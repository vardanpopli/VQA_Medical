{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7188971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da97e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "medidata = pd.read_excel(\"C:\\\\Users\\\\varda\\\\Documents\\\\mltask2\\\\VQA_RAD Dataset Public.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bafded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling the data \n",
    "\n",
    "sampledata = medidata[(medidata[\"ANSWER\"] == \"Yes\") | (medidata[\"ANSWER\"] == \"No\") | (medidata[\"ANSWER\"] == \"yes\") | (medidata[\"ANSWER\"] == \"no\") ]\n",
    "sampledata.reset_index(inplace = True, drop = True)\n",
    "#sampledata\n",
    "\n",
    "questions = sampledata.iloc[:,7].values\n",
    "answers = sampledata.iloc[:,12].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be8b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb57b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels\n",
    "labels = np.zeros((len(sampledata),))\n",
    "for i in range(len(sampledata)):\n",
    "    if (sampledata[\"ANSWER\"][i] == \"Yes\") | (sampledata[\"ANSWER\"][i] == \"yes\"):\n",
    "        labels[i] = 1\n",
    "    elif (sampledata[\"ANSWER\"][i] == \"No\") | (sampledata[\"ANSWER\"][i] == \"no\"):\n",
    "        labels[i] = 0\n",
    "    else:\n",
    "        labels[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0044c53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/varda/Documents/mltask2/VAQ image folder/synpic54610.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic29265.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic29265.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic29265.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic28602.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic28602.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic28602.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic42202.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic29265.jpg',\n",
       " 'C:/Users/varda/Documents/mltask2/VAQ image folder/synpic54610.jpg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image address\n",
    "IMGDIR = \"C:/Users/varda/Documents/mltask2/VAQ image folder/\"\n",
    "\n",
    "addresses = []\n",
    "for i in range(len(sampledata)):\n",
    "    addresses.append(IMGDIR + sampledata[\"IMAGEID\"][i][39:])\n",
    "addresses[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0195383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre-processing\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Define a function to load and preprocess image from a file path\n",
    "def preprocess_image(file_path):\n",
    "    # Read the image from the file\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_image(image, channels=3) \n",
    "\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    \n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load and preprocess all images from the list of file paths\n",
    "images = [preprocess_image(file_path) for file_path in addresses]\n",
    "\n",
    "# Convert the list of preprocessed images to a tensorflow tensor\n",
    "normalized_images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "\n",
    "# Now, normalized_images contains the images resized to 224x224 pixels and with pixel values normalized to [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0147d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation functions\n",
    "def augment_image(image):\n",
    "    # Randomly apply rotation\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    \n",
    "    image = tf.image.rot90(image, k=k)\n",
    "    \n",
    "    # Randomly flip the image horizontally\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    # Randomly adjust brightness (factor between 0.5 and 1.5)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Apply data augmentation to each image\n",
    "augmented_images = [augment_image(image) for image in images]\n",
    "\n",
    "# Convert the list of augmented images to a tensorflow tensor\n",
    "augmented_images_float = tf.convert_to_tensor(augmented_images, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d78bc656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897dadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize the questions and answers\n",
    "tokenized_data = []\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "    question_doc = nlp(question)\n",
    "    answer_doc = nlp(answer)\n",
    "    tokenized_item = {\n",
    "        \"question_tokens\": [token.text for token in question_doc],\n",
    "        \"answer_tokens\": [token.text for token in answer_doc],\n",
    "    }\n",
    "    tokenized_data.append(tokenized_item)\n",
    "    \n",
    "# Now, tokenized_data contains tokenized questions and answers for each item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19dbd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saperately tokenizing the questions and answers ans storing in the following list\n",
    "tokenized_questions = []\n",
    "tokenized_answers = []\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "    question_doc = nlp(question)\n",
    "    answer_doc = nlp(answer)\n",
    "    tokenized_questions.append([token.text for token in question_doc])\n",
    "    tokenized_answers.append([token.text for token in answer_doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d32a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ddd0ca33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor question_embeddings in word_embeddings:\\n    print(question_embeddings)\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Combine all tokens from questions and answers\n",
    "all_tokens = [item[\"question_tokens\"] + item[\"answer_tokens\"] for item in tokenized_data]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=all_tokens, vector_size=100, window=5, sg=0, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"cbow_Q_A_model.model\")\n",
    "model = Word2Vec.load(\"cbow_Q_A_model.model\")\n",
    "\n",
    "\n",
    "# word embeddings\n",
    "word_embeddings = {}\n",
    "\n",
    "for token in model.wv.index_to_key:\n",
    "    word_embeddings[token] = model.wv[token]\n",
    "\n",
    "# Now, word_embeddings contains Word2Vec embeddings for the tokens.\n",
    "\n",
    "'''\n",
    "for question_embeddings in word_embeddings:\n",
    "    print(question_embeddings)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3bdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "unique_words = list(word_embeddings.keys())\n",
    "\n",
    "# Create a vocabulary dictionary where each word is mapped to a unique ID\n",
    "word_to_id = {word: index for index, word in enumerate(unique_words)}\n",
    "\n",
    "# Now, 'word_to_id' is vocabulary mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ecdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ce6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define a directory where we will save the images\n",
    "output_directory = \"C:/Users/varda/Documents/mltask2/VQA_augmented_images_folder/\"  # Replace with your desired output directory\n",
    "\n",
    "# creating the output directory \n",
    "tf.io.gfile.makedirs(output_directory)\n",
    "\n",
    "# loop through the augmented images and save them to the output directory\n",
    "for i, image in enumerate(augmented_images):\n",
    "    image_bytes = tf.image.encode_jpeg(tf.cast(image * 255, tf.uint8))\n",
    "    filename = f\"augmented_image_{i}.jpg\"  # You can adjust the naming scheme\n",
    "    file_path = tf.strings.join([output_directory, filename])\n",
    "    tf.io.write_file(file_path, image_bytes)\n",
    "    #print(f\"Saved {filename} to {output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9386c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image address to load the images for feature extraction\n",
    "\n",
    "image_paths =[]\n",
    "for i in range (0,1192):\n",
    "\n",
    "    image_address = output_directory + f\"augmented_image_{i}.jpg\"\n",
    "    image_paths.append(image_address)\n",
    "    #print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d66a3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/varda/Documents/mltask2/VQA_augmented_images_folder/augmented_image_1191.jpg\n"
     ]
    }
   ],
   "source": [
    "print(image_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647bca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing image feature extraction\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# loading the pre-trained ResNet50 model \n",
    "resnet_model = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "resnet = hub.load(resnet_model)\n",
    "image_size = (224, 224)\n",
    "image_features = []\n",
    "\n",
    "\n",
    "for image_path in image_paths:\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)  # Ensure RGB images\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    img = tf.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    features = resnet(img)\n",
    "    image_features.append(features)\n",
    "\n",
    "# concatenate the image features\n",
    "image_features = tf.concat(image_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742bad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09991d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_features) #extra things for my information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c1e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "max_length = 0                 #extra things for my information\n",
    "for question in tokenized_questions:\n",
    "    length = len(question)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24d844bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(word_to_id.items()))    #extra things for my information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "408e73c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = list(word_embeddings.keys())    #extra things for my information\n",
    "len(dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f16a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39da4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the tokens, truncating the questions.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# create a tokenizer to map words to IDs\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_questions)\n",
    "\n",
    "# convert tokenized questions to sequences\n",
    "question_sequences = tokenizer.texts_to_sequences(tokenized_questions)\n",
    "\n",
    "max_sequence_length = 22\n",
    "\n",
    "padded_question_sequences = pad_sequences(question_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# convert the padded sequences to tensors\n",
    "question_tokens = tf.constant(padded_question_sequences)\n",
    "\n",
    "# Now, question_input contains the padded and truncated question sequences as tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8d28d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "image_input = tf.keras.Input(shape=(2048,), name='image_input')\n",
    "\n",
    "question_input = tf.keras.Input(shape=(22,), name='question_input')\n",
    "\n",
    "question_embedding = layers.Embedding(input_dim=849, output_dim=849)(question_input)\n",
    "\n",
    "# Add one or more LSTM layers for processing the question text\n",
    "question_lstm = LSTM(units=256, return_sequences=False)(question_embedding)\n",
    "\n",
    "# Concatenate the image features and question LSTM output\n",
    "concatenated_features = layers.concatenate([image_input, question_lstm])\n",
    "\n",
    "# Add one or more dense layers for joint understanding\n",
    "joint_dense = layers.Dense(512, activation='relu')(concatenated_features)\n",
    "\n",
    "output = layers.Dense(1, activation='softmax')(joint_dense)\n",
    "\n",
    "# Create the VQA model\n",
    "vqa_model = tf.keras.Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Compile the model with optimizer and loss function\n",
    "vqa_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62376220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting question and labesl for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "q = padded_question_sequences # Features: questions and answers\n",
    "y = labels  # Labels: anwers\n",
    "\n",
    "train_size = 0.7 \n",
    "validation_size = 0.15  \n",
    "test_size = 0.15  \n",
    "\n",
    "q_train, q_temp, y_train, y_temp = train_test_split(q, y, test_size=(1 - train_size), random_state=42)\n",
    "q_val, q_test, y_val, y_test = train_test_split(q_temp, y_temp, test_size=test_size / (1 - train_size), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f1b0a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting image features for training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the desired split percentages\n",
    "train_percent = 0.7\n",
    "validation_percent = 0.15\n",
    "test_percent = 0.15\n",
    "\n",
    "num_samples = len(image_features)\n",
    "num_train = int(train_percent * num_samples)\n",
    "num_validation = 179\n",
    "\n",
    "X_train, X_temp = image_features[:num_train], image_features[num_train:]\n",
    "X_val, X_test = X_temp[:num_validation], X_temp[num_validation:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31fd92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 6s 130ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 3s 118ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 3s 117ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 4s 134ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 3s 119ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 3s 126ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 3s 117ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 3s 129ms/step - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4916\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 0.5140\n",
      "Test Loss: 0.0, Test Accuracy: 0.5139665007591248\n"
     ]
    }
   ],
   "source": [
    "# finding Accuracy\n",
    "num_epochs = 10  \n",
    "batch_size = 32  \n",
    "\n",
    "history = vqa_model.fit(\n",
    "    [X_train, q_train], y_train,  # Provide image features, questions, and answers\n",
    "    validation_data= ([X_val, q_val], y_val),    epochs= num_epochs,  batch_size= batch_size)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = vqa_model.evaluate([X_test, q_test], y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "# hence, accuracy is comming out to be 0.5139\n",
    "# now after fine tuning, by applying different model eg. BERT, GPT-2 we can find more btter accuracy(I have tried, but didn't run)\n",
    "# so did it from scrach\n",
    "# or we can do more augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4538f56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834.4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = len(image_features)\n",
    "\n",
    "x*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "823cb992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d03af0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6de34c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d74b729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1191.4285714285716"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "834/0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "769edc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1112*0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52aedf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6996644295302014"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "834/1192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82405e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13993288590604025"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1600d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
